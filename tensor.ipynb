{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [2.],\n",
       "       [2.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#(2,1,2)-> two are a rows and columns here,and middle one is an axis  \n",
    "x=tf.ones(shape=(2,1,2))\n",
    "v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))\n",
    "#here 3 are a rows and 1 is a columns\n",
    "v.assign(tf.ones((3, 1)))\n",
    "#this function will add ones with ones \n",
    "v.assign_add(tf.ones((3, 1)))\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(9.8, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#using a gradient tape\n",
    "input_var=tf.Variable(initial_value=3.)\n",
    "# Use a gradient tape to track operations on the variable\n",
    "with tf.GradientTape() as tape:\n",
    "    result=tf.square(input_var)\n",
    "# Calculate the gradient of `result` with respect to `input_var`\n",
    "gradient=tape.gradient(result,input_var)\n",
    "\n",
    "#using a gradient tape\n",
    "input_const=tf.constant(6.)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(input_const)\n",
    "    result=tf.square(input_const)\n",
    "gradient=tape.gradient(result,input_const)\n",
    "\n",
    "time=tf.Variable(0.)\n",
    "with tf.GradientTape() as outerTape:\n",
    "    with tf.GradientTape() as innerTape:\n",
    "        position=4.9 * time ** 2\n",
    "    speed=innerTape.gradient(position,time)\n",
    "    print(speed)\n",
    "accelaration=outerTape.gradient(speed,time)\n",
    "print(accelaration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "num_samples_per_class=200\n",
    "#The multivariate normal, multinormal is a generalization of the one-dimensionalnormal distribution to  higher dimensions.\n",
    "negative_samples=np.random.multivariate_normal(\n",
    "    mean=[0,3],\n",
    "    cov=[[1,0.5],[0.5,1]],\n",
    "    size=num_samples_per_class\n",
    ")\n",
    "positive_samples=np.random.multivariate_normal(\n",
    "    mean=[3,0],\n",
    "    cov=[[1,0.5],[0.5,1]],\n",
    "    size=num_samples_per_class\n",
    ")\n",
    "inputs=np.vstack((negative_samples,positive_samples)).astype(np.float32)\n",
    "# Create the targets (0 for negative samples, 1 for positive samples)\n",
    "targets = np.vstack((\n",
    "    np.zeros((num_samples_per_class, 1), dtype=\"float32\"),  # Class 0 (negative)\n",
    "    np.ones((num_samples_per_class, 1), dtype=\"float32\")    # Class 1 (positive)\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 10:29:04.331697: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
      "2024-09-12 10:29:04.331718: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-09-12 10:29:04.331723: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-09-12 10:29:04.331907: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-12 10:29:04.331918: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 58\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#Steps\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#1.Initialization: Weights and bias , weights randomly and bias is initialized as 1.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#2.ForwardPass: It will give the predictions\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#5.Iteration:This process repeats many iterations , progressively reducing the loss score and improving the model predictions\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#The batch training loop\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m40\u001b[39m) :\n\u001b[0;32m---> 58\u001b[0m     loss\u001b[38;5;241m=\u001b[39m training_step(\u001b[43minputs\u001b[49m,targets)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "#This code is a simple example of how to implement supervised learning for a binary classification problem using linear regression and gradient descent in TensorFlow. \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# a basic linear regression model in TensorFlow to classify points in 2D space\n",
    "# linear regression model -a data analysis technique that predicts the value of unknown data by using another related\n",
    "# and known data value, It is a statistical method used in data science and machine learning for predictive analysis.\n",
    "\n",
    "# the input will be in 2D points \n",
    "input_dim=2 #Each sample has two features (e.g., x and y coordinates).\n",
    "# output predicted will be single score per sample.close to 0 if sample is in class 0 and close to 1 if sample is in class1\n",
    "# class 0  and class 1 is binary classification problem.where as class 0 is negative and class1 is positive\n",
    "\n",
    "output_dim=1  #The model outputs a single value, which is a prediction of the likelihood of a point belonging to Class 1.\n",
    "\n",
    "#W is an random number with shape as 2, 1 so 2 rows and 1 columns [[W1],[W2]]\n",
    "W=tf.Variable(tf.random.uniform(shape=(input_dim,output_dim)))\n",
    "\n",
    "#Whereas B is a zero with shape 1, array is [0.]\n",
    "# B is the bias term, initialized to zero, and it shifts the prediction result by a constant.\n",
    "b=tf.Variable(tf.zeros(shape=(output_dim)))\n",
    "\n",
    "# Forward pass inputs are [x,y]\n",
    "#This model function computes the predictional value using linear equations.\n",
    "def model(inputs):\n",
    "    return tf.matmul(inputs,W)+b # predictions = [[W1],[W2]] * [x,y] +b = W1 * x + w2 * y + b\n",
    "\n",
    "# Loss Function\n",
    "# The loss function is the mean squared error(MSE),which measures how far the predicted values are from the true targets (0 or 1).\n",
    "def square_loss(targets,predictions):\n",
    "    per_sample_losses= tf.square(targets-predictions)\n",
    "    return tf.reduce_mean(per_sample_losses)\n",
    "#Controls how much the weights and bias are updated during each training step. A higher learning rate leads to larger updates, \n",
    "#while a lower learning rate leads to smaller updates.\n",
    "learning_rate=0.1\n",
    "#The training step implements the optimization using gradient descent\n",
    "def training_step(inputs,targets):\n",
    "#tf.GradientTape is used to record the operations needed to compute the gradients of the loss with respect to the weights and bias.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions=model(inputs) #call forward pass function\n",
    "        loss=square_loss(predictions,targets) #call loss function\n",
    "    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss,[W,b])\n",
    "    print(grad_loss_wrt_W,\"grad_loss_wrt_W\")\n",
    "    print(grad_loss_wrt_b,\"grad_loss_wrt_b\")\n",
    "    #The weights and bias are updated by subtracting the product of the gradients and the learning rate\n",
    "    W.assign_sub(grad_loss_wrt_W * learning_rate)\n",
    "    b.assign_sub(grad_loss_wrt_b * learning_rate)\n",
    "    return loss\n",
    "\n",
    "#Steps\n",
    "#1.Initialization: Weights and bias , weights randomly and bias is initialized as 1.\n",
    "#2.ForwardPass: It will give the predictions\n",
    "#3.Loss function: MSE computes the difference between the predicted score and true value\n",
    "#4.Parameter Update: This model updates its weights and bias by calculating the gradients of the loss with respect to \n",
    "# the parameters and adjusting them in order to reduce the loss.\n",
    "#5.Iteration:This process repeats many iterations , progressively reducing the loss score and improving the model predictions\n",
    "#The batch training loop\n",
    "for step in range(40) :\n",
    "    loss= training_step(inputs,targets)\n",
    "    print(f\"Loss at step {step}: {loss:.4f}\")\n",
    "predictions = model(inputs)\n",
    "x=np.linspace(-1,4,100) #Return evenly spaced numbers over a specified interval.\n",
    "y=-W[0]/W[1] * x + (0.5-b) / W[1] # line equation\n",
    "plt.plot(x,y,\"-r\")\n",
    "plt.scatter(inputs[:,0],inputs[:,1],c=predictions[:,0]>0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
