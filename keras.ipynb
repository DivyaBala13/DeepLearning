{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32)\n"
     ]
    }
   ],
   "source": [
    "#Everything in keras is a layer or something that is related to layer\n",
    "#A layer is an object that encapsulates weights and some computation(#forward pass)\n",
    "#weights are defined in a build() and computation are defined in a call()\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "class SimpleDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation= None):\n",
    "        super().__init__()\n",
    "        self.units=units\n",
    "        self.activation=activation\n",
    "    \n",
    "    #weight creation takes place in the build() method.\n",
    "    def build(self, input_shape) :\n",
    "        input_dim= input_shape[-1]\n",
    "        #add_weights is an shorcut method for creating weights\n",
    "        self.W=self.add_weight(shape=(input_dim,self.units),initializer=\"random_normal\")\n",
    "        self.b=self.add_weight(shape=(self.units,),initializer=\"zeros\"\n",
    "        )\n",
    "    # we define the forward pass computation in the call method()\n",
    "    def call(self,inputs):\n",
    "        y=tf.matmul(inputs,self.W) + self.b\n",
    "        if self.activation is not None:\n",
    "            y=self.activation(y)\n",
    "        return y\n",
    "\n",
    "my_dense= SimpleDense(units=32,activation=tf.nn.relu) # instantiate our layer, defined previously.\n",
    "input_tensor= tf.ones(shape=(2,784)) # create some test inputs.\n",
    "output_tensor=my_dense(input_tensor) # call the layer on the inputs just like a function.\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Dense name=dense_3, built=False>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "#A dense layer with 32 output units\n",
    "layer=layers.Dense(32,activation=\"relu\")\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "models=models.Sequential([layers.Dense(32,activation=\"relu\"),\n",
    "                          layers.Dense(32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "#define alinera classifier\n",
    "model= keras.Sequential([keras.layers.Dense(1)])\n",
    "model.compile(optimizer=\"rmsprop\",loss=\"mean_squared_error\",metrics=[\"accuracy\"])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The fit method implements the training loop itself\n",
    "#1.The Data(inputs and targets)to train on.It is typically passed either in the form of numpy arrays and tensorflow datasets object.\n",
    "#2.The no of epochs to train for:how many times the training loop should iterate over the data passed.\n",
    "#3.The batch size to use within epochs of mini-batch gardient descent\n",
    "import numpy as np;\n",
    "num_samples_per_class=200\n",
    "#The multivariate normal, multinormal is a generalization of the one-dimensionalnormal distribution to  higher dimensions.\n",
    "negative_samples=np.random.multivariate_normal(\n",
    "    mean=[0,3],\n",
    "    cov=[[1,0.5],[0.5,1]],\n",
    "    size=num_samples_per_class\n",
    ")\n",
    "positive_samples=np.random.multivariate_normal(\n",
    "    mean=[3,0],\n",
    "    cov=[[1,0.5],[0.5,1]],\n",
    "    size=num_samples_per_class\n",
    ")\n",
    "inputs=np.vstack((negative_samples,positive_samples)).astype(np.float32)\n",
    "# Create the targets (0 for negative samples, 1 for positive samples)\n",
    "targets = np.vstack((\n",
    "    np.zeros((num_samples_per_class, 1), dtype=\"float32\"),  # Class 0 (negative)\n",
    "    np.ones((num_samples_per_class, 1), dtype=\"float32\")    # Class 1 (positive)\n",
    "))\n",
    "history = model.fit(inputs, targets,epochs=5,batch_size=128)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is using a tool called Keras, which helps us create a machine learning model. \n",
    "# A model is like a very smart machine that learns from data and can make predictions based on what it learned.\n",
    "\n",
    "\n",
    "# Here we are creating a very simple model. Think of the model as a brain that learns from examples. \n",
    "# It has one layer (a \"Dense\" layer) with 1 unit. You can think of a layer like a small part of the brain that helps it learn. \n",
    "# Since there’s only 1 unit, this brain is really simple and is likely used for making a decision between two things (like yes/no or true/false).\n",
    "model=keras.Sequential([keras.layers.Dense(1)])\n",
    "\n",
    "#When we compile the model, we’re telling it how to learn and how to measure how well it’s learning:\n",
    "\n",
    "#Optimizer (RMSprop): This is like the brain's thinking process. It tells the model how to adjust itself to learn better.\n",
    "#The learning rate (0.1) controls how fast it should learn.\n",
    "\n",
    "#Loss (MeanSquaredError): This measures how wrong the model's predictions are during learning. \n",
    "# The goal is to make the error as small as possible.\n",
    "\n",
    "#Metrics-BinaryAccuracy:This checks how many times the model's predictions are correct for two possible answers(like yes/no or 0/1).\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n",
    "              loss=keras.losses.MeanSquaredError(),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "#Before we teach the model, we want to mix (shuffle) the examples so that it doesn't learn them in a specific order.\n",
    "#Think of it like mixing flashcards before studying so you don’t memorize the order of answers.\n",
    "#np.random.permutation(len(inputs)): This shuffles the order of the data.\n",
    "#inputs are the examples we give to the model (like questions).\n",
    "#targets are the correct answers to those examples (like the answers to the questions).\n",
    "\n",
    "indices_permutations= np.random.permutation(len(inputs))\n",
    "shuffled_inputs= inputs[indices_permutations]\n",
    "shuffled_targets=targets[indices_permutations]\n",
    "\n",
    "#We split the data into two parts:\n",
    "\n",
    "#Training data (70% of examples): This is the data we use to teach the model.\n",
    "#Validation data (30% of examples): This is used to check how well the model is learning, but we don’t use it for teaching.\n",
    "num_validation_samples= int (0.3 * len(inputs))\n",
    "val_inputs=shuffled_inputs[:num_validation_samples]\n",
    "val_targets=shuffled_targets[:num_validation_samples]\n",
    "training_inputs= shuffled_inputs[num_validation_samples:]\n",
    "training_targets=shuffled_targets[num_validation_samples:]\n",
    "#Now, we’re actually teaching the model using the training data.\n",
    "\n",
    "#Epochs (5): This means the model will go through the entire training data 5 times to learn better.\n",
    "#Batch size (16): This means the model will look at 16 examples at a time while learning.\n",
    "#Validation data: While the model is learning, it checks its progress using the validation data to see how well it's doing.\n",
    "\n",
    "model.fit(\n",
    "    training_inputs,\n",
    "    training_targets,\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    validation_data=(val_inputs,val_targets)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"?thisfilmwasjustbrilliantcastinglocationscenerystorydirectioneveryone'sreallysuitedtheparttheyplayedandyoucouldjustimaginebeingthererobert?isanamazingactorandnowthesamebeingdirector?fathercamefromthesamescottishislandasmyselfsoilovedthefacttherewasarealconnectionwiththisfilmthewittyremarksthroughoutthefilmweregreatitwasjustbrilliantsomuchthatiboughtthefilmassoonasitwasreleasedfor?andwouldrecommendittoeveryonetowatchandtheflyfishingwasamazingreallycriedattheenditwassosadandyouknowwhattheysayifyoucryatafilmitmusthavebeengoodandthisdefinitelywasalso?tothetwolittleboy'sthatplayedthe?ofnormanandpaultheywerejustbrilliantchildrenareoftenleftoutofthe?listithinkbecausethestarsthatplaythemallgrownuparesuchabigprofileforthewholefilmbutthesechildrenareamazingandshouldbepraisedforwhattheyhavedonedon'tyouthinkthewholestorywassolovelybecauseitwastrueandwassomeone'slifeafterallthatwassharedwithusall\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(train_data,train_labels),(test_data,test_labels)= imdb.load_data(num_words=10000)\n",
    "train_data[0]\n",
    "train_labels[0]\n",
    "#This finds the highest word index used in the training dataset. Since you limited the vocabulary to the top 10,000 words, \n",
    "# the maximum index will be 9999 (because indices start from 0).\n",
    "max([max(sequence) for sequence in train_data])\n",
    "word_index= imdb.get_word_index()\n",
    "#This function returns a dictionary mapping words to integer indices\n",
    "#For example, word_index['the'] might return 1, meaning \"the\" is the most frequent word in the dataset.\n",
    "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n",
    "#This creates a reverse dictionary, reverse_word_index, where the keys are the integer indices and the values are the corresponding\n",
    "# words. This will be useful for decoding the review from integers back to words.\n",
    "decoded_review=\"\" .join([reverse_word_index.get(i-3,\"?\") for i in train_data[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#A list of sequences where each sequence is a list of integers (each integer represents a word index).\n",
    "#dimension: The size of the output vector. In this case, it's 10000 because we limited the vocabulary size to 10,000 words \n",
    "# when loading the IMDb dataset.\n",
    "def vectorize_sequences(sequences,dimension=10000):\n",
    "    results=np.zeros((len(sequences),dimension))\n",
    "    for i , sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i,j]= 1\n",
    "    return results\n",
    "x_train =vectorize_sequences(train_data)\n",
    "x_test=vectorize_sequences(test_data)\n",
    "\n",
    "y_train=np.asarray(train_labels).astype(\"float32\")\n",
    "y_test=np.asarray(test_labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5885 - loss: 0.2845 - val_accuracy: 0.7829 - val_loss: 0.1558\n",
      "Epoch 2/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8342 - loss: 0.1341 - val_accuracy: 0.8428 - val_loss: 0.1299\n",
      "Epoch 3/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8886 - loss: 0.1080 - val_accuracy: 0.8565 - val_loss: 0.1239\n",
      "Epoch 4/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9118 - loss: 0.0935 - val_accuracy: 0.8605 - val_loss: 0.1257\n",
      "Epoch 5/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9231 - loss: 0.0855 - val_accuracy: 0.8559 - val_loss: 0.1268\n",
      "Epoch 6/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9304 - loss: 0.0805 - val_accuracy: 0.8622 - val_loss: 0.1270\n",
      "Epoch 7/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9415 - loss: 0.0759 - val_accuracy: 0.8653 - val_loss: 0.1239\n",
      "Epoch 8/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9456 - loss: 0.0717 - val_accuracy: 0.8612 - val_loss: 0.1282\n",
      "Epoch 9/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9487 - loss: 0.0707 - val_accuracy: 0.8620 - val_loss: 0.1278\n",
      "Epoch 10/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9558 - loss: 0.0658 - val_accuracy: 0.8561 - val_loss: 0.1311\n",
      "Epoch 11/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9599 - loss: 0.0624 - val_accuracy: 0.8586 - val_loss: 0.1326\n",
      "Epoch 12/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9601 - loss: 0.0614 - val_accuracy: 0.8546 - val_loss: 0.1369\n",
      "Epoch 13/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9647 - loss: 0.0604 - val_accuracy: 0.8517 - val_loss: 0.1384\n",
      "Epoch 14/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9608 - loss: 0.0605 - val_accuracy: 0.8539 - val_loss: 0.1377\n",
      "Epoch 15/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9653 - loss: 0.0581 - val_accuracy: 0.8509 - val_loss: 0.1420\n",
      "Epoch 16/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9643 - loss: 0.0568 - val_accuracy: 0.8504 - val_loss: 0.1419\n",
      "Epoch 17/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9680 - loss: 0.0564 - val_accuracy: 0.8430 - val_loss: 0.1513\n",
      "Epoch 18/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9684 - loss: 0.0568 - val_accuracy: 0.8443 - val_loss: 0.1476\n",
      "Epoch 19/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9664 - loss: 0.0567 - val_accuracy: 0.8445 - val_loss: 0.1478\n",
      "Epoch 20/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9728 - loss: 0.0531 - val_accuracy: 0.8260 - val_loss: 0.1670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['accuracy', 'loss', 'val_accuracy', 'val_loss']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "model=keras.Sequential([layers.Dense(16,activation=\"relu\"),\n",
    "                        layers.Dense(16,activation=\"relu\"),\n",
    "                        layers.Dense(1,activation=\"sigmoid\")])\n",
    "model= keras.Sequential([keras.layers.Dense(1)])\n",
    "model.compile(optimizer=\"rmsprop\",loss=\"mean_squared_error\",metrics=[\"accuracy\"])\n",
    "\n",
    "x_val=x_train[:10000]\n",
    "partial_x_train= x_train[10000:]\n",
    "y_val=y_train[:10000]\n",
    "partial_y_train=y_train[10000:]\n",
    "\n",
    "history=model.fit(partial_x_train,\n",
    "                  partial_y_train,\n",
    "                  epochs=20,\n",
    "                  batch_size=512,\n",
    "                  validation_data=(x_val,y_val))\n",
    "history_dict =history.history\n",
    "history_dict.keys()\n",
    "[u\"accuracy\",u\"loss\",u\"val_accuracy\",u\"val_loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict =history.history\n",
    "loss_values=history_dict[\"loss\"]\n",
    "val_loss_value=history_dict[\"val_loss\"]\n",
    "epochs= range(1,len(loss_values)+1)\n",
    "plt.plot(epochs,loss_values,\"bo\",label=\"Training loss\")\n",
    "plt.plot(epochs,val_loss_value,\"b\",label=\"Validation Loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function \n",
    "#categorical_crossentropy - It measures the distance between two probability distributions,between the probability distribution\n",
    "# output by the model and the true distribution of the labels.By minimizing the distance between these two distributions , you\n",
    "# train the model to input something as close as possible to the true label .\n",
    "\n",
    "#Information bottleneck:\n",
    "#If one layer drops some information relevant to the classification problem , this information can never be recovered by later \n",
    "# layers:each layer can pottentially become an information bottleneck.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
